# Generated by jinja based on the template dag_with_kpo_template.j2.
# Don't modify it unless you know what you are doing.

# mypy: ignore-errors
import os

from modules.config.accumulation_dag_config import accumulation_jobs
from modules.util.dag_utils import (
    dag_failure_callback,
    dag_success_callback,
    kpo_failure_callback,
    kpo_success_callback,
)
from modules.util.kube_utils import get_env_vars, get_image_name, get_metadata_labels
from modules.util.scheduling_utils import should_run_task
from pendulum import datetime, duration

from airflow import models
from airflow.operators.python import ShortCircuitOperator
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator

with models.DAG(
    dag_id="accumulation_file_transfer_dag",
    params={
        "team_ns": "payments_platform",
        "service_ns": "payer_accumulation",
        "payers": [
            job["payer"] for job in accumulation_jobs if "file_transfer" in job["jobs"]
        ],
        "run_off_schedule": False,
    },
    schedule="0 21,18,15,12 * * *",
    catchup=False,
    start_date=datetime(2025, 1, 21),
    tags=["payments_platform", "payer_accumulation", "in_mono"],
    on_success_callback=dag_success_callback,
    on_failure_callback=dag_failure_callback,
) as dag:
    base_task_id = "accumulation_file_transfer_job"
    pod_template_file = "/home/airflow/gcs/" + "plugins/mono_api_pod_spec_file.yaml"

    image_name = get_image_name()
    image_tag = image_name.split(":")[1]
    gcp_project = os.environ.get("GCP_PROJECT")

    # values shared across all kubernetes pod operator tasks
    kpo_constants = dict(
        namespace="mvn-airflow-job",
        labels=get_metadata_labels(image_tag),
        image=image_name,
        env_vars=get_env_vars(),
        config_file="/home/airflow/composer_kube_config",
        kubernetes_conn_id="kubernetes_default",
        pod_template_file=pod_template_file,
        is_delete_operator_pod=True,
        startup_timeout_seconds=600,
        log_pod_spec_on_failure=False,
        log_events_on_failure=True,
        retries=2,
        retry_delay=duration(seconds=300),
        on_success_callback=kpo_success_callback,
        on_failure_callback=kpo_failure_callback,
    )
    base_command = ["python3", "-c"]
    # missing: task_id, name, cmds

    get_launchdarkly_config = KubernetesPodOperator(
        task_id="get_launchdarkly_config",
        cmds=[
            "python3",
            "-c",
            "from airflow.scripts.payments_platform.accumulation_helper_scripts import get_consolidated_accumulation_file_transfer_feature_flag; "
            "get_consolidated_accumulation_file_transfer_feature_flag()",
        ],
        do_xcom_push=True,
        **kpo_constants,
    )

    # create the tasks dynamically based on which payers we want to run
    file_transfer_jobs = [
        job for job in accumulation_jobs if "file_transfer" in job["jobs"]
    ]
    default_payers = [job["payer"] for job in file_transfer_jobs]
    for job in file_transfer_jobs:
        condition_task = ShortCircuitOperator(
            task_id=f"{job['payer']}-check_condition",
            python_callable=should_run_task,
            provide_context=True,
            op_kwargs={
                "payer": job["payer"],
                "schedule": job["job_schedules"]["file_transfer"].value,
                "job_type": "file_transfer",
                "default_payers": default_payers,
            },
        )

        command = f"from airflow.scripts.payments_platform.accumulation_file_transfer import {job['payer']}_accumulation_file_transfer_job; {job['payer']}_accumulation_file_transfer_job()"
        file_transfer_task = KubernetesPodOperator(
            task_id=f"{job['payer']}-{base_task_id}",
            name=f"mono-for-{job['payer']}-{base_task_id}",
            cmds=base_command + [command],
            **kpo_constants,
        )

        get_launchdarkly_config >> condition_task >> file_transfer_task
